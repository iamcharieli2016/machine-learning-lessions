#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¢¯åº¦ä¸‹é™æ³•ç®€åŒ–æ¼”ç¤º
Simple Gradient Descent Demo

è¿™æ˜¯ä¸€ä¸ªæœ€ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™å®ç°ï¼Œç”¨äºç†è§£æ ¸å¿ƒæ¦‚å¿µ
"""

import numpy as np
import matplotlib.pyplot as plt

def simple_gradient_descent_demo():
    """
    æœ€ç®€å•çš„æ¢¯åº¦ä¸‹é™æ¼”ç¤º
    ç›®æ ‡ï¼šæ‰¾åˆ°å‡½æ•° f(x) = (x-3)Â² çš„æœ€å°å€¼ç‚¹
    """
    print("ğŸ¯ ç®€å•æ¢¯åº¦ä¸‹é™æ¼”ç¤ºï¼šå¯»æ‰¾å‡½æ•° f(x) = (x-3)Â² çš„æœ€å°å€¼")
    print("=" * 50)
    
    # å®šä¹‰ç›®æ ‡å‡½æ•°å’Œå…¶å¯¼æ•°
    def f(x):
        return (x - 3) ** 2
    
    def df_dx(x):  # å¯¼æ•°
        return 2 * (x - 3)
    
    # åˆå§‹åŒ–å‚æ•°
    x = 0.0  # èµ·å§‹ç‚¹
    learning_rate = 0.1
    epochs = 20
    
    # è®°å½•ä¼˜åŒ–è¿‡ç¨‹
    x_history = [x]
    f_history = [f(x)]
    
    print(f"èµ·å§‹ç‚¹: x = {x:.3f}, f(x) = {f(x):.3f}")
    print("å¼€å§‹æ¢¯åº¦ä¸‹é™...")
    print("-" * 30)
    
    # æ¢¯åº¦ä¸‹é™è¿­ä»£
    for epoch in range(epochs):
        # è®¡ç®—æ¢¯åº¦
        gradient = df_dx(x)
        
        # æ›´æ–°å‚æ•°
        x = x - learning_rate * gradient
        
        # è®°å½•è¿‡ç¨‹
        x_history.append(x)
        f_history.append(f(x))
        
        print(f"ç¬¬{epoch+1:2d}è½®: x = {x:.3f}, f(x) = {f(x):.3f}, æ¢¯åº¦ = {gradient:.3f}")
    
    print("-" * 30)
    print(f"æœ€ç»ˆç»“æœ: x = {x:.3f}, f(x) = {f(x):.6f}")
    print(f"ç†è®ºæœ€ä¼˜è§£: x = 3.000, f(x) = 0.000000")
    
    # å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
    plt.figure(figsize=(12, 5))
    
    # å·¦å›¾ï¼šå‡½æ•°æ›²çº¿å’Œä¼˜åŒ–è·¯å¾„
    plt.subplot(1, 2, 1)
    x_range = np.linspace(-1, 6, 100)
    y_range = f(x_range)
    plt.plot(x_range, y_range, 'b-', linewidth=2, label='f(x) = (x-3)Â²')
    
    # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„
    for i in range(len(x_history)-1):
        plt.arrow(x_history[i], f_history[i], 
                 x_history[i+1] - x_history[i], 
                 f_history[i+1] - f_history[i],
                 head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.7)
    
    plt.scatter(x_history, f_history, color='red', s=50, zorder=5)
    plt.scatter([3], [0], color='green', s=100, marker='*', label='å…¨å±€æœ€ä¼˜è§£')
    plt.title('æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è·¯å¾„')
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šå‚æ•°å’Œå‡½æ•°å€¼çš„å˜åŒ–
    plt.subplot(1, 2, 2)
    epochs_range = range(len(x_history))
    plt.plot(epochs_range, x_history, 'r-o', label='å‚æ•° x', markersize=4)
    plt.plot(epochs_range, f_history, 'b-s', label='å‡½æ•°å€¼ f(x)', markersize=4)
    plt.title('ä¼˜åŒ–è¿‡ç¨‹ä¸­å‚æ•°å’Œå‡½æ•°å€¼çš„å˜åŒ–')
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('æ•°å€¼')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def linear_regression_step_by_step():
    """
    é€æ­¥æ¼”ç¤ºçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
    """
    print("\nğŸ¯ çº¿æ€§å›å½’æ¢¯åº¦ä¸‹é™é€æ­¥æ¼”ç¤º")
    print("=" * 50)
    
    # ç”Ÿæˆç®€å•æ•°æ®
    np.random.seed(42)
    x = np.array([1, 2, 3, 4, 5])
    y = np.array([2, 4, 6, 8, 10])  # å®Œç¾çš„çº¿æ€§å…³ç³» y = 2x
    
    print(f"è®­ç»ƒæ•°æ®: ")
    for i in range(len(x)):
        print(f"  x[{i}] = {x[i]}, y[{i}] = {y[i]}")
    
    # åˆå§‹åŒ–å‚æ•°
    w = 0.0  # æƒé‡
    b = 0.0  # åç½®
    learning_rate = 0.1
    
    print(f"\nåˆå§‹å‚æ•°: w = {w}, b = {b}")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("-" * 60)
    
    # è®­ç»ƒå‡ æ­¥ï¼Œè¯¦ç»†å±•ç¤ºæ¯ä¸€æ­¥
    for step in range(5):
        print(f"\nç¬¬ {step+1} æ­¥:")
        
        # 1. å‰å‘ä¼ æ’­
        y_pred = w * x + b
        print(f"  1. å‰å‘ä¼ æ’­: y_pred = {w:.3f} * x + {b:.3f}")
        print(f"     é¢„æµ‹å€¼: {y_pred}")
        
        # 2. è®¡ç®—æŸå¤±
        loss = np.mean((y_pred - y) ** 2) / 2
        print(f"  2. è®¡ç®—æŸå¤±: MSE = {loss:.6f}")
        
        # 3. è®¡ç®—æ¢¯åº¦
        n = len(x)
        dw = np.mean((y_pred - y) * x)
        db = np.mean(y_pred - y)
        print(f"  3. è®¡ç®—æ¢¯åº¦: dw = {dw:.6f}, db = {db:.6f}")
        
        # 4. æ›´æ–°å‚æ•°
        w_new = w - learning_rate * dw
        b_new = b - learning_rate * db
        print(f"  4. æ›´æ–°å‚æ•°: w = {w:.3f} - {learning_rate} * {dw:.6f} = {w_new:.6f}")
        print(f"                b = {b:.3f} - {learning_rate} * {db:.6f} = {b_new:.6f}")
        
        w, b = w_new, b_new
        print(f"  æ›´æ–°åå‚æ•°: w = {w:.6f}, b = {b:.6f}")
    
    print(f"\næœ€ç»ˆå‚æ•°: w = {w:.6f}, b = {b:.6f}")
    print(f"ç†è®ºæœ€ä¼˜è§£: w = 2.000000, b = 0.000000")

if __name__ == "__main__":
    # è¿è¡Œç®€å•æ¼”ç¤º
    simple_gradient_descent_demo()
    
    # è¿è¡Œçº¿æ€§å›å½’é€æ­¥æ¼”ç¤º
    linear_regression_step_by_step()
