# ReLU神经网络训练样例

本目录包含了ReLU激活函数神经网络的完整实现和演示代码，展示了从Sigmoid到ReLU的改进。

## 文件说明

### 1. `relu_neural_network.py` - 完整版ReLU神经网络
- **功能**: 完整的ReLU神经网络实现，支持任意层数和神经元数量
- **特点**: 
  - 使用He初始化权重
  - 批次训练
  - 完整的可视化功能
  - 10000个训练样本

### 2. `simple_relu_demo.py` - 简化版演示
- **功能**: 简化的ReLU网络演示，便于理解核心概念
- **特点**:
  - 代码结构清晰
  - 1000个训练样本
  - 快速演示

## 快速开始

### 运行简化版演示
```bash
python simple_relu_demo.py
```

### 运行完整版演示
```bash
python relu_neural_network.py
```

## 核心概念

### ReLU激活函数
```python
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)
```

### 网络结构（图中公式对应）
```
输入层 → 隐藏层1(ReLU) → 隐藏层2(ReLU) → 输出层(Sigmoid)
```

对应图中的公式：
```
y = b + Σ cᵢ max(0, bᵢ + Σ wᵢⱼxⱼ)
```

### ReLU相比Sigmoid的优势

1. **计算简单**: `max(0, x)` vs 复杂的指数运算
2. **缓解梯度消失**: 正区间导数恒为1
3. **稀疏激活**: 负值被抑制为0，提高效率
4. **更好的收敛性**: 训练速度更快

## 实验结果

### 数据集
- **样本数量**: 10000个（完整版）/ 1000个（简化版）
- **特征维度**: 2维
- **类别数量**: 2类（二分类问题）

### 网络配置
- **完整版**: 2 → 64 → 32 → 1
- **简化版**: 2 → 10 → 1

### 训练参数
- **学习率**: 0.01（完整版）/ 0.1（简化版）
- **批次大小**: 64（完整版）/ 全批次（简化版）
- **训练轮数**: 500（完整版）/ 1000（简化版）

## 可视化内容

1. **激活函数比较**: Sigmoid vs ReLU
2. **数据分布**: 原始训练数据的分布
3. **决策边界**: 网络学习到的分类边界
4. **训练过程**: 损失函数和准确率的变化

## 技术要点

### He初始化
```python
w = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)
```
- 适合ReLU激活函数
- 有效防止梯度消失和爆炸

### 反向传播中的ReLU梯度
```python
# ReLU导数只在正区间为1，负区间为0
relu_grad = (z > 0).astype(float)
```

### 损失函数
- 使用二元交叉熵损失
- 适合二分类问题

## 扩展练习

1. **尝试不同的网络结构**
   - 增加或减少隐藏层数量
   - 调整每层的神经元数量

2. **实验不同的学习率**
   - 观察收敛速度的变化
   - 找到最优的学习率

3. **比较其他激活函数**
   - Leaky ReLU
   - ELU (Exponential Linear Unit)
   - Swish

4. **处理更复杂的数据集**
   - 多分类问题
   - 高维数据

## 依赖包
```bash
pip install numpy matplotlib
```

## 运行环境
- Python 3.6+
- NumPy
- Matplotlib

---

这个实现展示了现代深度学习中ReLU激活函数的重要性，以及它如何解决传统Sigmoid函数的梯度消失问题。
