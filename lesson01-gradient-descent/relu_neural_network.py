#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ReLUç¥ç»ç½‘ç»œè®­ç»ƒæ ·ä¾‹
ReLU Neural Network Training Example

å®ç°ä¸€ä¸ªä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„å¤šå±‚ç¥ç»ç½‘ç»œï¼Œæ¼”ç¤ºä»Sigmoidåˆ°ReLUçš„æ”¹è¿›
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ReLUNeuralNetwork:
    """
    ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„å¤šå±‚ç¥ç»ç½‘ç»œ
    
    ç½‘ç»œç»“æ„ï¼š
    - è¾“å…¥å±‚ â†’ éšè—å±‚1 (ReLU) â†’ éšè—å±‚2 (ReLU) â†’ è¾“å‡ºå±‚
    - æ”¯æŒä»»æ„éšè—å±‚å¤§å°å’Œå±‚æ•°
    """
    
    def __init__(self, layer_sizes: List[int], learning_rate: float = 0.01):
        """
        åˆå§‹åŒ–ç¥ç»ç½‘ç»œ
        
        Args:
            layer_sizes: æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡ï¼Œä¾‹å¦‚ [2, 64, 32, 1]
            learning_rate: å­¦ä¹ ç‡
        """
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.num_layers = len(layer_sizes)
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = []
        self.biases = []
        
        # Heåˆå§‹åŒ–ï¼Œé€‚åˆReLUæ¿€æ´»å‡½æ•°
        for i in range(self.num_layers - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
        
        # è®°å½•è®­ç»ƒè¿‡ç¨‹
        self.loss_history = []
        self.accuracy_history = []
        
        print(f"ç¥ç»ç½‘ç»œåˆå§‹åŒ–å®Œæˆ:")
        print(f"ç½‘ç»œç»“æ„: {' â†’ '.join(map(str, layer_sizes))}")
        print(f"å­¦ä¹ ç‡: {learning_rate}")
        print(f"æƒé‡åˆå§‹åŒ–: Heåˆå§‹åŒ– (é€‚åˆReLU)")
    
    def relu(self, x: np.ndarray) -> np.ndarray:
        """ReLUæ¿€æ´»å‡½æ•°: f(x) = max(0, x)"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x: np.ndarray) -> np.ndarray:
        """ReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°"""
        return (x > 0).astype(float)
    
    def sigmoid(self, x: np.ndarray) -> np.ndarray:
        """Sigmoidæ¿€æ´»å‡½æ•°ï¼ˆç”¨äºè¾“å‡ºå±‚ï¼‰"""
        # é˜²æ­¢æ•°å€¼æº¢å‡º
        x = np.clip(x, -500, 500)
        return 1 / (1 + np.exp(-x))
    
    def forward_pass(self, X: np.ndarray) -> Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            X: è¾“å…¥æ•°æ® (batch_size, input_features)
            
        Returns:
            output: ç½‘ç»œè¾“å‡º
            activations: æ¯å±‚çš„æ¿€æ´»å€¼
            z_values: æ¯å±‚çš„çº¿æ€§å˜æ¢ç»“æœï¼ˆæ¿€æ´»å‰ï¼‰
        """
        activations = [X]  # ç¬¬0å±‚æ˜¯è¾“å…¥
        z_values = []
        
        current_input = X
        
        # é€šè¿‡æ‰€æœ‰éšè—å±‚
        for i in range(self.num_layers - 2):
            # çº¿æ€§å˜æ¢: z = X * W + b
            z = np.dot(current_input, self.weights[i]) + self.biases[i]
            z_values.append(z)
            
            # ReLUæ¿€æ´»
            a = self.relu(z)
            activations.append(a)
            current_input = a
        
        # è¾“å‡ºå±‚ï¼ˆä½¿ç”¨sigmoidè¿›è¡ŒäºŒåˆ†ç±»ï¼‰
        z_output = np.dot(current_input, self.weights[-1]) + self.biases[-1]
        z_values.append(z_output)
        output = self.sigmoid(z_output)
        activations.append(output)
        
        return output, activations, z_values
    
    def compute_loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è®¡ç®—äºŒå…ƒäº¤å‰ç†µæŸå¤±"""
        # é˜²æ­¢log(0)
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
        return loss
    
    def backward_pass(self, X: np.ndarray, y: np.ndarray, 
                     activations: List[np.ndarray], z_values: List[np.ndarray]) -> None:
        """
        åå‘ä¼ æ’­ç®—æ³•
        
        Args:
            X: è¾“å…¥æ•°æ®
            y: çœŸå®æ ‡ç­¾
            activations: å‰å‘ä¼ æ’­çš„æ¿€æ´»å€¼
            z_values: å‰å‘ä¼ æ’­çš„çº¿æ€§å˜æ¢ç»“æœ
        """
        m = X.shape[0]  # batch size
        
        # è®¡ç®—æ¢¯åº¦
        dW = [np.zeros_like(w) for w in self.weights]
        db = [np.zeros_like(b) for b in self.biases]
        
        # è¾“å‡ºå±‚è¯¯å·®ï¼ˆsigmoid + äº¤å‰ç†µæŸå¤±çš„æ¢¯åº¦ï¼‰
        delta = activations[-1] - y
        
        # ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­
        for i in range(self.num_layers - 2, -1, -1):
            # è®¡ç®—å½“å‰å±‚çš„æƒé‡å’Œåç½®æ¢¯åº¦
            dW[i] = np.dot(activations[i].T, delta) / m
            db[i] = np.mean(delta, axis=0, keepdims=True)
            
            # å¦‚æœä¸æ˜¯è¾“å…¥å±‚ï¼Œè®¡ç®—ä¸‹ä¸€å±‚çš„è¯¯å·®
            if i > 0:
                # è¯¯å·®åå‘ä¼ æ’­
                delta = np.dot(delta, self.weights[i].T)
                # åº”ç”¨ReLUå¯¼æ•°
                delta = delta * self.relu_derivative(z_values[i-1])
        
        # æ›´æ–°æƒé‡å’Œåç½®
        for i in range(self.num_layers - 1):
            self.weights[i] -= self.learning_rate * dW[i]
            self.biases[i] -= self.learning_rate * db[i]
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, 
              batch_size: int = 32, verbose: bool = True) -> None:
        """
        è®­ç»ƒç¥ç»ç½‘ç»œ
        
        Args:
            X: è®­ç»ƒæ•°æ®
            y: è®­ç»ƒæ ‡ç­¾
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            verbose: æ˜¯å¦æ‰“å°è®­ç»ƒè¿›åº¦
        """
        n_samples = X.shape[0]
        n_batches = n_samples // batch_size
        
        print(f"\nå¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œ...")
        print(f"è®­ç»ƒæ ·æœ¬: {n_samples}, æ‰¹æ¬¡å¤§å°: {batch_size}, è®­ç»ƒè½®æ•°: {epochs}")
        print("=" * 60)
        
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_loss = 0
            epoch_accuracy = 0
            
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            # æ‰¹æ¬¡è®­ç»ƒ
            for batch_idx in range(n_batches):
                start_idx = batch_idx * batch_size
                end_idx = start_idx + batch_size
                
                X_batch = X_shuffled[start_idx:end_idx]
                y_batch = y_shuffled[start_idx:end_idx]
                
                # å‰å‘ä¼ æ’­
                y_pred, activations, z_values = self.forward_pass(X_batch)
                
                # è®¡ç®—æŸå¤±
                batch_loss = self.compute_loss(y_pred, y_batch)
                epoch_loss += batch_loss
                
                # è®¡ç®—å‡†ç¡®ç‡
                predictions = (y_pred > 0.5).astype(int)
                batch_accuracy = np.mean(predictions == y_batch)
                epoch_accuracy += batch_accuracy
                
                # åå‘ä¼ æ’­
                self.backward_pass(X_batch, y_batch, activations, z_values)
            
            # è®°å½•å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_loss = epoch_loss / n_batches
            avg_accuracy = epoch_accuracy / n_batches
            
            self.loss_history.append(avg_loss)
            self.accuracy_history.append(avg_accuracy)
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            if verbose and (epoch + 1) % 100 == 0:
                elapsed_time = time.time() - start_time
                print(f"Epoch {epoch+1:4d}/{epochs}: "
                      f"Loss = {avg_loss:.6f}, "
                      f"Accuracy = {avg_accuracy:.4f}, "
                      f"Time = {elapsed_time:.2f}s")
        
        total_time = time.time() - start_time
        print("=" * 60)
        print(f"è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        print(f"æœ€ç»ˆæŸå¤±: {self.loss_history[-1]:.6f}")
        print(f"æœ€ç»ˆå‡†ç¡®ç‡: {self.accuracy_history[-1]:.4f}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        output, _, _ = self.forward_pass(X)
        return output
    
    def predict_classes(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        predictions = self.predict(X)
        return (predictions > 0.5).astype(int)
    
    def plot_training_history(self):
        """å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(self.loss_history, 'b-', linewidth=2)
        ax1.set_title('è®­ç»ƒæŸå¤±å˜åŒ–')
        ax1.set_xlabel('è®­ç»ƒè½®æ•°')
        ax1.set_ylabel('æŸå¤±å€¼')
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(self.accuracy_history, 'r-', linewidth=2)
        ax2.set_title('è®­ç»ƒå‡†ç¡®ç‡å˜åŒ–')
        ax2.set_xlabel('è®­ç»ƒè½®æ•°')
        ax2.set_ylabel('å‡†ç¡®ç‡')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()


def generate_classification_data(n_samples: int = 10000, random_seed: int = 42) -> Tuple[np.ndarray, np.ndarray]:
    """
    ç”ŸæˆäºŒåˆ†ç±»æ•°æ®é›†
    
    Args:
        n_samples: æ ·æœ¬æ•°é‡
        random_seed: éšæœºç§å­
        
    Returns:
        X: ç‰¹å¾æ•°æ® (n_samples, 2)
        y: æ ‡ç­¾æ•°æ® (n_samples, 1)
    """
    np.random.seed(random_seed)
    
    # ç”Ÿæˆä¸¤ä¸ªç±»åˆ«çš„æ•°æ®
    n_class0 = n_samples // 2
    n_class1 = n_samples - n_class0
    
    # ç±»åˆ«0ï¼šä»¥(-1, -1)ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒ
    X_class0 = np.random.multivariate_normal([-1, -1], [[0.8, 0.2], [0.2, 0.8]], n_class0)
    y_class0 = np.zeros((n_class0, 1))
    
    # ç±»åˆ«1ï¼šä»¥(1, 1)ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒ
    X_class1 = np.random.multivariate_normal([1, 1], [[0.8, -0.2], [-0.2, 0.8]], n_class1)
    y_class1 = np.ones((n_class1, 1))
    
    # åˆå¹¶æ•°æ®
    X = np.vstack([X_class0, X_class1])
    y = np.vstack([y_class0, y_class1])
    
    # éšæœºæ‰“ä¹±
    indices = np.random.permutation(n_samples)
    X = X[indices]
    y = y[indices]
    
    print(f"ç”Ÿæˆåˆ†ç±»æ•°æ®é›†: {n_samples} ä¸ªæ ·æœ¬, 2ä¸ªç‰¹å¾, 2ä¸ªç±»åˆ«")
    print(f"ç±»åˆ«0: {n_class0} ä¸ªæ ·æœ¬")
    print(f"ç±»åˆ«1: {n_class1} ä¸ªæ ·æœ¬")
    
    return X, y


def visualize_data_and_results(X: np.ndarray, y: np.ndarray, model: ReLUNeuralNetwork):
    """å¯è§†åŒ–æ•°æ®å’Œæ¨¡å‹ç»“æœ"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('ReLUç¥ç»ç½‘ç»œè®­ç»ƒç»“æœ', fontsize=16)
    
    # 1. åŸå§‹æ•°æ®åˆ†å¸ƒ
    ax = axes[0, 0]
    class0_mask = y.flatten() == 0
    class1_mask = y.flatten() == 1
    
    ax.scatter(X[class0_mask, 0], X[class0_mask, 1], c='red', alpha=0.6, label='ç±»åˆ« 0')
    ax.scatter(X[class1_mask, 0], X[class1_mask, 1], c='blue', alpha=0.6, label='ç±»åˆ« 1')
    ax.set_title('åŸå§‹æ•°æ®åˆ†å¸ƒ')
    ax.set_xlabel('ç‰¹å¾ 1')
    ax.set_ylabel('ç‰¹å¾ 2')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. å†³ç­–è¾¹ç•Œ
    ax = axes[0, 1]
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    mesh_points = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(mesh_points)
    Z = Z.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
    ax.scatter(X[class0_mask, 0], X[class0_mask, 1], c='red', alpha=0.8, label='ç±»åˆ« 0')
    ax.scatter(X[class1_mask, 0], X[class1_mask, 1], c='blue', alpha=0.8, label='ç±»åˆ« 1')
    ax.set_title('å†³ç­–è¾¹ç•Œ')
    ax.set_xlabel('ç‰¹å¾ 1')
    ax.set_ylabel('ç‰¹å¾ 2')
    ax.legend()
    
    # 3. è®­ç»ƒæŸå¤±
    ax = axes[1, 0]
    ax.plot(model.loss_history, 'b-', linewidth=2)
    ax.set_title('è®­ç»ƒæŸå¤±å˜åŒ–')
    ax.set_xlabel('è®­ç»ƒè½®æ•°')
    ax.set_ylabel('æŸå¤±å€¼')
    ax.grid(True, alpha=0.3)
    
    # 4. è®­ç»ƒå‡†ç¡®ç‡
    ax = axes[1, 1]
    ax.plot(model.accuracy_history, 'r-', linewidth=2)
    ax.set_title('è®­ç»ƒå‡†ç¡®ç‡å˜åŒ–')
    ax.set_xlabel('è®­ç»ƒè½®æ•°')
    ax.set_ylabel('å‡†ç¡®ç‡')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def compare_activation_functions():
    """æ¯”è¾ƒSigmoidå’ŒReLUæ¿€æ´»å‡½æ•°"""
    x = np.linspace(-5, 5, 100)
    
    # Sigmoidå‡½æ•°
    sigmoid = 1 / (1 + np.exp(-x))
    sigmoid_derivative = sigmoid * (1 - sigmoid)
    
    # ReLUå‡½æ•°
    relu = np.maximum(0, x)
    relu_derivative = (x > 0).astype(float)
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle('æ¿€æ´»å‡½æ•°æ¯”è¾ƒ: Sigmoid vs ReLU', fontsize=16)
    
    # Sigmoidå‡½æ•°
    axes[0, 0].plot(x, sigmoid, 'b-', linewidth=2, label='Sigmoid')
    axes[0, 0].set_title('Sigmoidæ¿€æ´»å‡½æ•°')
    axes[0, 0].set_xlabel('è¾“å…¥å€¼')
    axes[0, 0].set_ylabel('è¾“å‡ºå€¼')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].legend()
    
    # ReLUå‡½æ•°
    axes[0, 1].plot(x, relu, 'r-', linewidth=2, label='ReLU')
    axes[0, 1].set_title('ReLUæ¿€æ´»å‡½æ•°')
    axes[0, 1].set_xlabel('è¾“å…¥å€¼')
    axes[0, 1].set_ylabel('è¾“å‡ºå€¼')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].legend()
    
    # Sigmoidå¯¼æ•°
    axes[1, 0].plot(x, sigmoid_derivative, 'b--', linewidth=2, label='Sigmoidå¯¼æ•°')
    axes[1, 0].set_title('Sigmoidå¯¼æ•°')
    axes[1, 0].set_xlabel('è¾“å…¥å€¼')
    axes[1, 0].set_ylabel('å¯¼æ•°å€¼')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].legend()
    
    # ReLUå¯¼æ•°
    axes[1, 1].plot(x, relu_derivative, 'r--', linewidth=2, label='ReLUå¯¼æ•°')
    axes[1, 1].set_title('ReLUå¯¼æ•°')
    axes[1, 1].set_xlabel('è¾“å…¥å€¼')
    axes[1, 1].set_ylabel('å¯¼æ•°å€¼')
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].legend()
    
    plt.tight_layout()
    plt.show()
    
    print("æ¿€æ´»å‡½æ•°ç‰¹ç‚¹æ¯”è¾ƒ:")
    print("Sigmoid:")
    print("  ä¼˜ç‚¹: è¾“å‡ºèŒƒå›´(0,1), å¹³æ»‘å¯å¯¼")
    print("  ç¼ºç‚¹: æ¢¯åº¦æ¶ˆå¤±é—®é¢˜, è®¡ç®—å¤æ‚")
    print("ReLU:")
    print("  ä¼˜ç‚¹: è®¡ç®—ç®€å•, ç¼“è§£æ¢¯åº¦æ¶ˆå¤±, ç¨€ç–æ¿€æ´»")
    print("  ç¼ºç‚¹: ç¥ç»å…ƒæ­»äº¡é—®é¢˜")


def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„ReLUç¥ç»ç½‘ç»œè®­ç»ƒæ¼”ç¤º"""
    print("ğŸ¯ ReLUç¥ç»ç½‘ç»œè®­ç»ƒæ¼”ç¤º")
    print("=" * 60)
    
    # 1. æ¯”è¾ƒæ¿€æ´»å‡½æ•°
    print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šæ¯”è¾ƒSigmoidå’ŒReLUæ¿€æ´»å‡½æ•°")
    compare_activation_functions()
    
    # 2. ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("\nğŸ“Š ç¬¬äºŒæ­¥ï¼šç”Ÿæˆè®­ç»ƒæ•°æ®")
    X_train, y_train = generate_classification_data(n_samples=10000, random_seed=42)
    
    # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
    print("\nğŸš€ ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºå’Œè®­ç»ƒReLUç¥ç»ç½‘ç»œ")
    # ç½‘ç»œç»“æ„: 2ä¸ªè¾“å…¥ â†’ 64ä¸ªéšè—å•å…ƒ â†’ 32ä¸ªéšè—å•å…ƒ â†’ 1ä¸ªè¾“å‡º
    model = ReLUNeuralNetwork(layer_sizes=[2, 64, 32, 1], learning_rate=0.01)
    
    # è®­ç»ƒæ¨¡å‹
    model.train(X_train, y_train, epochs=500, batch_size=64, verbose=True)
    
    # 4. è¯„ä¼°æ¨¡å‹
    print("\nğŸ“‹ ç¬¬å››æ­¥ï¼šæ¨¡å‹è¯„ä¼°")
    train_predictions = model.predict_classes(X_train)
    train_accuracy = np.mean(train_predictions == y_train)
    print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f}")
    
    # 5. å¯è§†åŒ–ç»“æœ
    print("\nğŸ“ˆ ç¬¬äº”æ­¥ï¼šå¯è§†åŒ–è®­ç»ƒç»“æœ")
    visualize_data_and_results(X_train, y_train, model)
    
    # 6. ç½‘ç»œç»“æ„ä¿¡æ¯
    print("\nğŸ—ï¸ ç½‘ç»œç»“æ„è¯¦æƒ…:")
    total_params = 0
    for i, (w, b) in enumerate(zip(model.weights, model.biases)):
        layer_params = w.size + b.size
        total_params += layer_params
        print(f"  ç¬¬{i+1}å±‚: æƒé‡{w.shape}, åç½®{b.shape}, å‚æ•°æ•°é‡: {layer_params}")
    
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params}")
    
    print("\nâœ… ReLUç¥ç»ç½‘ç»œè®­ç»ƒæ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ å…³é”®æ”¹è¿›ç‚¹:")
    print("   1. ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
    print("   2. Heåˆå§‹åŒ–é€‚åˆReLUç½‘ç»œ")
    print("   3. æ‰¹æ¬¡è®­ç»ƒæé«˜è®­ç»ƒæ•ˆç‡")
    print("   4. æ·±å±‚ç½‘ç»œå¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»")


if __name__ == "__main__":
    main()
