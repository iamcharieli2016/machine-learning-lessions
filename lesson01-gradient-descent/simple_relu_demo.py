#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆReLUç¥ç»ç½‘ç»œæ¼”ç¤º
Simple ReLU Neural Network Demo

å¿«é€Ÿæ¼”ç¤ºReLUç¥ç»ç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µå’Œè®­ç»ƒè¿‡ç¨‹
"""

import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def relu(x):
    """ReLUæ¿€æ´»å‡½æ•°"""
    return np.maximum(0, x)

def relu_derivative(x):
    """ReLUå¯¼æ•°"""
    return (x > 0).astype(float)

def sigmoid(x):
    """Sigmoidæ¿€æ´»å‡½æ•°"""
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def simple_relu_network_demo():
    """ç®€å•çš„ReLUç½‘ç»œæ¼”ç¤º"""
    print("ğŸ¯ ç®€å•ReLUç¥ç»ç½‘ç»œæ¼”ç¤º")
    print("=" * 50)
    
    # ç”Ÿæˆç®€å•çš„äºŒåˆ†ç±»æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    # ç±»åˆ«0: åœ†å½¢åŒºåŸŸå†…çš„ç‚¹
    angles = np.random.uniform(0, 2*np.pi, n_samples//2)
    radius = np.random.uniform(0, 1, n_samples//2)
    X_class0 = np.column_stack([
        radius * np.cos(angles),
        radius * np.sin(angles)
    ])
    y_class0 = np.zeros((n_samples//2, 1))
    
    # ç±»åˆ«1: åœ†ç¯åŒºåŸŸçš„ç‚¹
    angles = np.random.uniform(0, 2*np.pi, n_samples//2)
    radius = np.random.uniform(1.5, 2.5, n_samples//2)
    X_class1 = np.column_stack([
        radius * np.cos(angles),
        radius * np.sin(angles)
    ])
    y_class1 = np.ones((n_samples//2, 1))
    
    # åˆå¹¶æ•°æ®
    X = np.vstack([X_class0, X_class1])
    y = np.vstack([y_class0, y_class1])
    
    # éšæœºæ‰“ä¹±
    indices = np.random.permutation(n_samples)
    X, y = X[indices], y[indices]
    
    print(f"ç”Ÿæˆæ•°æ®: {n_samples} ä¸ªæ ·æœ¬ï¼Œ2ä¸ªç‰¹å¾ï¼Œ2ä¸ªç±»åˆ«")
    
    # ç½‘ç»œå‚æ•°
    input_size = 2
    hidden_size = 10
    output_size = 1
    learning_rate = 0.1
    epochs = 1000
    
    # åˆå§‹åŒ–æƒé‡ï¼ˆHeåˆå§‹åŒ–ï¼‰
    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)
    b1 = np.zeros((1, hidden_size))
    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)
    b2 = np.zeros((1, output_size))
    
    print(f"ç½‘ç»œç»“æ„: {input_size} â†’ {hidden_size} (ReLU) â†’ {output_size} (Sigmoid)")
    print(f"å­¦ä¹ ç‡: {learning_rate}, è®­ç»ƒè½®æ•°: {epochs}")
    
    # è®­ç»ƒè¿‡ç¨‹è®°å½•
    losses = []
    accuracies = []
    
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("-" * 40)
    
    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­
        # éšè—å±‚
        z1 = np.dot(X, W1) + b1
        a1 = relu(z1)
        
        # è¾“å‡ºå±‚
        z2 = np.dot(a1, W2) + b2
        a2 = sigmoid(z2)
        
        # è®¡ç®—æŸå¤±ï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰
        loss = -np.mean(y * np.log(np.clip(a2, 1e-15, 1-1e-15)) + 
                       (1 - y) * np.log(np.clip(1 - a2, 1e-15, 1-1e-15)))
        losses.append(loss)
        
        # è®¡ç®—å‡†ç¡®ç‡
        predictions = (a2 > 0.5).astype(int)
        accuracy = np.mean(predictions == y)
        accuracies.append(accuracy)
        
        # åå‘ä¼ æ’­
        # è¾“å‡ºå±‚æ¢¯åº¦
        dz2 = a2 - y
        dW2 = np.dot(a1.T, dz2) / n_samples
        db2 = np.mean(dz2, axis=0, keepdims=True)
        
        # éšè—å±‚æ¢¯åº¦
        da1 = np.dot(dz2, W2.T)
        dz1 = da1 * relu_derivative(z1)
        dW1 = np.dot(X.T, dz1) / n_samples
        db1 = np.mean(dz1, axis=0, keepdims=True)
        
        # æ›´æ–°å‚æ•°
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 200 == 0:
            print(f"Epoch {epoch+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}")
    
    print("-" * 40)
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}, æœ€ç»ˆå‡†ç¡®ç‡: {accuracies[-1]:.4f}")
    
    # å¯è§†åŒ–ç»“æœ
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('ç®€å•ReLUç¥ç»ç½‘ç»œè®­ç»ƒç»“æœ', fontsize=16)
    
    # 1. åŸå§‹æ•°æ®
    ax = axes[0, 0]
    class0_mask = y.flatten() == 0
    class1_mask = y.flatten() == 1
    ax.scatter(X[class0_mask, 0], X[class0_mask, 1], c='red', alpha=0.6, label='ç±»åˆ« 0', s=20)
    ax.scatter(X[class1_mask, 0], X[class1_mask, 1], c='blue', alpha=0.6, label='ç±»åˆ« 1', s=20)
    ax.set_title('åŸå§‹æ•°æ®åˆ†å¸ƒ')
    ax.set_xlabel('ç‰¹å¾ 1')
    ax.set_ylabel('ç‰¹å¾ 2')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. å†³ç­–è¾¹ç•Œ
    ax = axes[0, 1]
    h = 0.1
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # é¢„æµ‹ç½‘æ ¼ç‚¹
    mesh_points = np.c_[xx.ravel(), yy.ravel()]
    z1_mesh = relu(np.dot(mesh_points, W1) + b1)
    z2_mesh = sigmoid(np.dot(z1_mesh, W2) + b2)
    Z = z2_mesh.reshape(xx.shape)
    
    ax.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
    ax.scatter(X[class0_mask, 0], X[class0_mask, 1], c='red', alpha=0.8, label='ç±»åˆ« 0', s=20)
    ax.scatter(X[class1_mask, 0], X[class1_mask, 1], c='blue', alpha=0.8, label='ç±»åˆ« 1', s=20)
    ax.set_title('å­¦ä¹ åˆ°çš„å†³ç­–è¾¹ç•Œ')
    ax.set_xlabel('ç‰¹å¾ 1')
    ax.set_ylabel('ç‰¹å¾ 2')
    ax.legend()
    
    # 3. æŸå¤±æ›²çº¿
    ax = axes[1, 0]
    ax.plot(losses, 'b-', linewidth=2)
    ax.set_title('è®­ç»ƒæŸå¤±å˜åŒ–')
    ax.set_xlabel('è®­ç»ƒè½®æ•°')
    ax.set_ylabel('æŸå¤±å€¼')
    ax.grid(True, alpha=0.3)
    
    # 4. å‡†ç¡®ç‡æ›²çº¿
    ax = axes[1, 1]
    ax.plot(accuracies, 'r-', linewidth=2)
    ax.set_title('è®­ç»ƒå‡†ç¡®ç‡å˜åŒ–')
    ax.set_xlabel('è®­ç»ƒè½®æ•°')
    ax.set_ylabel('å‡†ç¡®ç‡')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return W1, b1, W2, b2

def compare_activations_simple():
    """ç®€å•æ¯”è¾ƒæ¿€æ´»å‡½æ•°"""
    print("\nğŸ” æ¿€æ´»å‡½æ•°æ¯”è¾ƒ")
    print("-" * 30)
    
    x = np.linspace(-3, 3, 100)
    
    # è®¡ç®—å‡½æ•°å€¼
    sigmoid_vals = sigmoid(x)
    relu_vals = relu(x)
    
    # è®¡ç®—å¯¼æ•°
    sigmoid_derivs = sigmoid_vals * (1 - sigmoid_vals)
    relu_derivs = relu_derivative(x)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # å‡½æ•°æ¯”è¾ƒ
    axes[0].plot(x, sigmoid_vals, 'b-', linewidth=2, label='Sigmoid')
    axes[0].plot(x, relu_vals, 'r-', linewidth=2, label='ReLU')
    axes[0].set_title('æ¿€æ´»å‡½æ•°æ¯”è¾ƒ')
    axes[0].set_xlabel('è¾“å…¥å€¼')
    axes[0].set_ylabel('è¾“å‡ºå€¼')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # å¯¼æ•°æ¯”è¾ƒ
    axes[1].plot(x, sigmoid_derivs, 'b--', linewidth=2, label='Sigmoidå¯¼æ•°')
    axes[1].plot(x, relu_derivs, 'r--', linewidth=2, label='ReLUå¯¼æ•°')
    axes[1].set_title('å¯¼æ•°æ¯”è¾ƒ')
    axes[1].set_xlabel('è¾“å…¥å€¼')
    axes[1].set_ylabel('å¯¼æ•°å€¼')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("ReLUç›¸æ¯”Sigmoidçš„ä¼˜åŠ¿:")
    print("1. è®¡ç®—ç®€å•ï¼šmax(0, x)")
    print("2. ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼šæ­£åŒºé—´å¯¼æ•°ä¸º1")
    print("3. ç¨€ç–æ¿€æ´»ï¼šè´Ÿå€¼è¢«æŠ‘åˆ¶ä¸º0")
    print("4. æ›´å¥½çš„æ”¶æ•›æ€§èƒ½")

if __name__ == "__main__":
    # è¿è¡Œæ¿€æ´»å‡½æ•°æ¯”è¾ƒ
    compare_activations_simple()
    
    # è¿è¡Œç®€å•æ¼”ç¤º
    simple_relu_network_demo()
    
    print("\nâœ… ç®€å•ReLUç¥ç»ç½‘ç»œæ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ è¦ç‚¹æ€»ç»“:")
    print("   â€¢ ReLUæ¿€æ´»å‡½æ•°: f(x) = max(0, x)")
    print("   â€¢ è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
    print("   â€¢ è®¡ç®—æ•ˆç‡é«˜ï¼Œè®­ç»ƒé€Ÿåº¦å¿«")
    print("   â€¢ é€‚åˆæ·±å±‚ç¥ç»ç½‘ç»œ")
